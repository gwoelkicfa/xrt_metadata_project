{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from os import listdir\n",
    "from os import walk\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sunpy\n",
    "import sunpy.map\n",
    "import pyfits\n",
    "import csv\n",
    "from StringIO import StringIO\n",
    "import requests\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getXrtPointings(time1,time2,first=True):\n",
    "    '''Return a list of metadata indicating the location and size of XRT's field of view during each pointing\n",
    "    in the specified time period. Returns list of lists: [[xcen,ycen,naxis1,naxis2,plate_scl],...]. Works across\n",
    "    day, month, and year boundaries between time1 and time2.\n",
    "    \n",
    "    Arguments:\n",
    "    time1 -- Initial time boundary (datetime object)\n",
    "    time2 -- Final time boundary. Must be after time1 (datetime object)\n",
    "    \n",
    "    Keyword arguments'''\n",
    "    \n",
    "    # Create list of filepaths to data\n",
    "    years = []\n",
    "    for year in listdir('/archive/hinode/xrt/level0/'):\n",
    "        if year >= str(time1.year) and year <= str(time2.year):\n",
    "            years.append(year)\n",
    "    \n",
    "    months = []\n",
    "    for year in years:\n",
    "        gen = (month for month in listdir('/archive/hinode/xrt/level0/' + year) if month in ['01','02','03','04','05','06','07','08','09','10','11','12'])\n",
    "        for month in gen:\n",
    "            time = datetime.datetime.strptime(month+year,'%m%Y')\n",
    "            if time >= time1.replace(day=1,hour=0,minute=0,second=0,microsecond=0) and time <= time2.replace(day=1,hour=0,minute=0,second=0,microsecond=0):\n",
    "                months.append(year + '/' + month)\n",
    "    \n",
    "    days = []\n",
    "    for month in months:\n",
    "        for day in listdir('/archive/hinode/xrt/level0/' + month):\n",
    "            time = datetime.datetime.strptime(month+day,'%Y/%m%d')\n",
    "            if time >= time1.replace(hour=0,minute=0,second=0,microsecond=0) and time <= time2.replace(hour=0,minute=0,second=0,microsecond=0):\n",
    "                days.append(month + '/' + day)\n",
    "    \n",
    "    hours = []\n",
    "    for day in days:\n",
    "        for hour in listdir('/archive/hinode/xrt/level0/' + day):\n",
    "            time = datetime.datetime.strptime(day + hour,'%Y/%m/%dH%H00')\n",
    "            if time >= time1.replace(minute=0,second=0,microsecond=0) and time <= time2.replace(minute=0,second=0,microsecond=0):\n",
    "                hours.append(day + '/' + hour)\n",
    "        \n",
    "    metadata = [] #list of lists: [[xcen,ycen,naxis1,naxis2,plate_scl],...]\n",
    "    \n",
    "    # Append the FOV metadata from the first science image in each data directory to the list metadata\n",
    "    for hour in hours:\n",
    "        for root, dirs, files in walk('/archive/hinode/xrt/level0/' + hour):\n",
    "            for e in files:\n",
    "                time = datetime.datetime.strptime(e[:18],'XRT%Y%m%d_%H%M%S')\n",
    "                \n",
    "                if time > time1 and time < time2:\n",
    "                    a = pyfits.open(root + '/' + e)\n",
    "                    \n",
    "                    if a[0].header['chip_sum'] != 8:\n",
    "                        if (a[0].header['ec_fw2_'] != 'Gband') and (a[0].header['ec_fw1_'] != a[0].header['ec_fw2_']):\n",
    "                            if first:\n",
    "                                return [[a[0].header['xcen'],a[0].header['ycen'],a[0].header['naxis1'],\n",
    "                                         a[0].header['naxis2'],a[0].header['platescl']]];\n",
    "                            \n",
    "                            metadata.append([a[0].header['xcen'],a[0].header['ycen'],a[0].header['naxis1'],\n",
    "                                             a[0].header['naxis2'],a[0].header['platescl']])\n",
    "                    a.close()\n",
    "    return metadata;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def groupFlareResData(flares):\n",
    "    '''Return flare response data grouped together by location -- improves readability of annotated synoptics.\n",
    "    Return list of lists - each list has averaged xcen, ycen, naxis1, naxis2, and platescl data, plus a final\n",
    "    element indicating the number of group members.\n",
    "    \n",
    "    Arguments:\n",
    "    flares -- A list of lists in the format [[xcen,ycen,naxis1,naxis2,plate_scl],...]'''\n",
    "    \n",
    "    flareGroups = []\n",
    "    \n",
    "    for flare in flares:\n",
    "        inGroup = False\n",
    "        for group in flareGroups:\n",
    "            # Condition that determines grouping\n",
    "            if abs(flare[0] - group[0]/group[6]) < 150 and abs(flare[1] - group[1]/group[6]) < 150:\n",
    "                inGroup = True\n",
    "                for i in range(6):\n",
    "                    group[i] = group[i] + flare[i]\n",
    "                group[6] = group[6] + 1\n",
    "        if not inGroup:\n",
    "            flare.append(1)\n",
    "            flareGroups.append(flare)\n",
    "    \n",
    "    # Average together metadata values\n",
    "    for group in flareGroups:\n",
    "        for i in range(5):\n",
    "            group[i] = group[i]/group[6]\n",
    "    \n",
    "    return flareGroups\n",
    "\n",
    "def groupFlareCatData(flares):\n",
    "    '''Return Hinode Flare Catalog data grouped by flare locations. Return list of group lists in the following\n",
    "    format: [[num,x_ave,y_ave,class,class,class...],...]\n",
    "    \n",
    "    Argument:\n",
    "    flares -- List of tuples for flares in the desired time range: [(t_start,x,y,class),...]'''\n",
    "    \n",
    "    flareGroups = []\n",
    "    \n",
    "    for flare in flares:\n",
    "        inGroup = False\n",
    "        for group in flareGroups:\n",
    "            # Condition that determines grouping\n",
    "            if abs(flare[1]-group[1]/group[0]) < 200 and abs(flare[2]-group[2]/group[0]) < 200:\n",
    "                inGroup = True\n",
    "                group[0] = group[0] + 1\n",
    "                group[1] = group[1] + flare[1]\n",
    "                group[2] = group[2] + flare[2]\n",
    "                group.append(flare[3])\n",
    "        if not inGroup:\n",
    "            flareGroups.append([1,flare[1],flare[2],flare[3]])\n",
    "\n",
    "    # Average together x and y coordinates\n",
    "    for group in flareGroups:\n",
    "        group[1] = group[1]/group[0]\n",
    "        group[2] = group[2]/group[0]\n",
    "    \n",
    "    return flareGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_and_purge(in_str, delim=None, purge=[\"\", \"+\"]):\n",
    "    '''Split a string into an array of strings, removing unwanted strings.\n",
    "    \n",
    "    Arguments:\n",
    "    in_str -- The string that is to be split\n",
    "    \n",
    "    Keyword arguments:\n",
    "    delim -- Character(s) used to break lines (default None)\n",
    "    purge -- A list of strings to be purged (default ['','+'])'''\n",
    "    \n",
    "    pieces = []\n",
    "    \n",
    "    if delim is None:\n",
    "        pieces = in_str.split()  # Split for both spaces and tabs.\n",
    "    else:\n",
    "        pieces = in_str.split(delim)\n",
    "    \n",
    "    return [piece for piece in pieces if piece not in purge]\n",
    "\n",
    "def parse_events(events_file):\n",
    "    \"\"\"Parse a Hinode pointing file into a pandas-friendly form.\"\"\"\n",
    "    # Split into lines.\n",
    "    lines = split_and_purge(events_file, delim=\"\\n\")\n",
    "\n",
    "    # Throw away the first 8 lines. We do not need the info contained in them.\n",
    "    lines = lines[8:]\n",
    "\n",
    "    if len(lines) < 2:\n",
    "        return \"\"\n",
    "\n",
    "    # Set column titles\n",
    "    title_row = ['Repoint','Start Time', 'Tracking Curve','Offset-X','Offset-Y','Xpointing','Ypointing','Notes']\n",
    "\n",
    "    # Get the rows\n",
    "    rows = []\n",
    "    for line in lines[:]:\n",
    "        items = split_and_purge(line)\n",
    "\n",
    "        # Too few items.\n",
    "        if len(items) < 7:\n",
    "            continue\n",
    "\n",
    "        # The .join combines 2 items into one (for example making the day and time into one row)\n",
    "        rows.append([\" \".join(items[:2])] + [\" \".join(items[2:4])] + items[4:7] + items[8:10]+[\" \".join(items[10:])])\n",
    "\n",
    "    # Create csv.\n",
    "    strio = StringIO()\n",
    "    writer = csv.writer(strio)\n",
    "    writer.writerows([title_row] + rows)\n",
    "    return strio.getvalue()\n",
    "\n",
    "def parseTimelineInfo(page):\n",
    "    lines = split_and_purge(page, delim='\\n')\n",
    "    lines = lines[11:-5]\n",
    "    output = []\n",
    "    for line in lines[:]:\n",
    "        if 'Plan' in line:\n",
    "            output.append(line.replace('>','<').split('<')[4])\n",
    "    return output\n",
    "\n",
    "def parseFlareResInfo(page):\n",
    "    '''Returns document as a list of tuples of datetime objects: [(t_start,t_end),...]'''\n",
    "    lines = split_and_purge(page,delim='\\n')\n",
    "    output = []\n",
    "    for line in lines[7:]:\n",
    "        times = line.split()\n",
    "        t1 = datetime.datetime.strptime(times[0],'%Y-%m-%dT%H:%M:%S')\n",
    "        t2 = datetime.datetime.strptime(times[1],'%Y-%m-%dT%H:%M:%S')\n",
    "        output.append((t1,t2))\n",
    "    return output;\n",
    "\n",
    "def parseHinodeFlareCat(f):\n",
    "    '''Returns a list of tuples (tStart,x,y,class) for all flares in catalogue'''\n",
    "    lines = split_and_purge(f,delim='\\n')\n",
    "    output = []\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        row = line.split(',')\n",
    "        try:\n",
    "            output.append((datetime.datetime.strptime(row[1],'%Y/%m/%d %H:%M'),int(row[6]),int(row[7]),row[5]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPointingFileName(page):\n",
    "    lines = split_and_purge(page, delim='\\n')\n",
    "    for line in lines:\n",
    "        if 're-point_' in line:\n",
    "            return line.replace('re-point_','.txt').split('.txt')[1]\n",
    "\n",
    "def getPointingFileInfo(time,offset):\n",
    "    '''Arguments are a datetime object and a timedelta object, respectively\n",
    "    Returns list of pandas dataframes'''\n",
    "    \n",
    "    t = time.replace(minute=0,second=0,microsecond=0)\n",
    "    timelines = urllib2.urlopen('https://xrt.cfa.harvard.edu/missionops/timelines/')\n",
    "    strio = StringIO(requests.get('https://xrt.cfa.harvard.edu/missionops/timelines/').content)\n",
    "    timelineFile = strio.read()\n",
    "    info = parseTimelineInfo(timelineFile)\n",
    "    \n",
    "    dates = []\n",
    "    for date in info:\n",
    "        dates.append(datetime.datetime.strptime(date[:10],'%Y/%m/%d'))\n",
    "    \n",
    "    fileDates = []\n",
    "    \n",
    "    i = 0\n",
    "    while fileDates == []: #sequential search for starting date of relevant pointing file\n",
    "        if dates[i]-(t-offset) < datetime.datetime.resolution:\n",
    "            fileDates.append(dates[i])\n",
    "            if i > 0:\n",
    "                fileDates.append(dates[i-1])\n",
    "            if i < len(dates):\n",
    "                fileDates.append(dates[i+1])\n",
    "        i = i + 1\n",
    "    \n",
    "    output = []\n",
    "    for date in fileDates:\n",
    "        url = 'http://xrt.cfa.harvard.edu/missionops/timelines/' + date.strftime('%Y%m%d') + '_Plan/' \n",
    "        webPage = urllib2.urlopen(url)\n",
    "        strio = StringIO(requests.get(url).content)\n",
    "        url = 'http://xrt.cfa.harvard.edu/missionops/timelines/' + date.strftime('%Y%m%d') + '_Plan/re-point_' + getPointingFileName(strio.read())+'.txt'\n",
    "        f = urllib2.urlopen(url)\n",
    "        strio = StringIO(requests.get(url).content)\n",
    "        output.append(pd.read_csv(StringIO(parse_events(strio.read()))))\n",
    "        f.close()\n",
    "        webPage.close()\n",
    "        del strio\n",
    "    \n",
    "    timelines.close()\n",
    "    \n",
    "    return output;\n",
    "\n",
    "def getFlareModeInfo(time1,time2):\n",
    "    '''Arguments are datetime objects. time1 must be prior to time2\n",
    "    Returns list of tuples of times (t_start,t_end) of flare responses between time1 and time2'''\n",
    "    t = time.time()\n",
    "    f = urllib2.urlopen('https://xrt.cfa.harvard.edu/missionops/flare_trigger_list/xrt_flare_responses.txt')\n",
    "    strio = StringIO(requests.get('https://xrt.cfa.harvard.edu/missionops/flare_trigger_list/xrt_flare_responses.txt').content)\n",
    "    flareFile = strio.read()\n",
    "    info = parseFlareResInfo(flareFile)\n",
    "    \n",
    "    output = []\n",
    "    for flare in info:\n",
    "        if time1<flare[0] and flare[0]<time2:\n",
    "            output.append(flare)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return output;\n",
    "\n",
    "def getHinodeFlareCat(time1,time2):\n",
    "    '''Arguments are datetime objects. time1 must be priot to time2\n",
    "    returns list of tuples: [(tStart,x,y,class),...]'''\n",
    "    f = urllib2.urlopen('http://st4a.stelab.nagoya-u.ac.jp/hinode_flare/csv/all_event.csv')\n",
    "    strio = StringIO(requests.get('http://st4a.stelab.nagoya-u.ac.jp/hinode_flare/csv/all_event.csv').content)\n",
    "    catalog = strio.read()\n",
    "    info = parseHinodeFlareCat(catalog)\n",
    "    \n",
    "    output = []\n",
    "    for e in info:\n",
    "        if time1 < e[0] and e[0] < time2:\n",
    "            output.append(e)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSynoptics(time1,time2):\n",
    "    '''Return a list of paths to synoptic FITS files (list of strings).\n",
    "    \n",
    "    Arguments:\n",
    "    time1 -- Initial time boundary (datetime object)\n",
    "    time2 -- Final time boundary (datetime object)'''\n",
    "    \n",
    "    # Generate directory paths\n",
    "    years = []\n",
    "    for year in listdir('/archive/hinode/xrt/level2/synoptics/'):\n",
    "        if year >= str(time1.year) and year <= str(time2.year):\n",
    "            years.append(year)\n",
    "    \n",
    "    months = []\n",
    "    for year in years:\n",
    "        gen = (month for month in listdir('/archive/hinode/xrt/level2/synoptics/' + year) if month in ['01','02','03','04','05','06','07','08','09','10','11','12'])\n",
    "        for month in gen:\n",
    "            time = datetime.datetime.strptime(month+year,'%m%Y')\n",
    "            if time >= time1.replace(day=1,hour=0,minute=0,second=0,microsecond=0) and time <= time2.replace(day=1,hour=0,minute=0,second=0,microsecond=0):\n",
    "                months.append(year + '/' + month)\n",
    "    \n",
    "    days = []\n",
    "    for month in months:\n",
    "        for day in listdir('/archive/hinode/xrt/level2/synoptics/'+ month):\n",
    "            time = datetime.datetime.strptime(month+day,'%Y/%m%d')\n",
    "            if time >= time1.replace(hour=0,minute=0,second=0,microsecond=0) and time <= time2.replace(hour=0,minute=0,second=0,microsecond=0):\n",
    "                days.append(month + '/' + day)\n",
    "    \n",
    "    hours = []\n",
    "    for day in days:\n",
    "        for hour in listdir('/archive/hinode/xrt/level2/synoptics/'+ day):\n",
    "            time = datetime.datetime.strptime(day+hour,'%Y/%m/%dH%H00')\n",
    "            if time >= time1.replace(minute=0,second=0,microsecond=0) and time <= time2.replace(minute=0,second=0,microsecond=0):\n",
    "                hours.append(day + '/' + hour)\n",
    "                \n",
    "    paths = []\n",
    "    \n",
    "    # Generate file paths\n",
    "    for hour in hours:\n",
    "        for root, dirs, files in walk('/archive/hinode/xrt/level2/synoptics/'+hour):\n",
    "            for e in files:\n",
    "                if e[-5:] == '.fits':\n",
    "                    f = pyfits.open(root + '/' + e)\n",
    "                    # Return only Al_mesh synoptics - makes video less jarring\n",
    "                    if f[0].header['ec_fw2_'] == 'Al_mesh':\n",
    "                        paths.append(root + '/' + e)\n",
    "                    f.close()\n",
    "                 \n",
    "    return paths;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displaySynoptic(path):\n",
    "    '''Display the requested synoptics with annotations and boxes.\n",
    "    \n",
    "    Arguments:\n",
    "    path -- String of the file's path'''\n",
    "\n",
    "    # Open file\n",
    "    file1 = pyfits.open(path)\n",
    "    \n",
    "    date_obs = datetime.datetime.strptime(file1[0].header['date_obs'],'%Y-%m-%dT%H:%M:%S.%f')\n",
    "    offset = datetime.timedelta(hours=6)\n",
    "    \n",
    "    dfList = getPointingFileInfo(date_obs,offset)\n",
    "    pointingInfo = [] # list of tuples: [(startTime,trackingCurve),...]\n",
    "    for df in dfList:\n",
    "        pointingInfo = pointingInfo + [(datetime.datetime.strptime(time,'%Y/%m/%d %H:%M:%S'),curve) for time,curve,\n",
    "                                       repoint,x,y,notes in zip(df['Start Time'],df['Tracking Curve'],df['Repoint'],\n",
    "                                                                df['Xpointing'],df['Ypointing'],\n",
    "                                                                df['Notes']) if (repoint == 'ORe-point Start' and \n",
    "                                                                                 'quadrant' not in notes)]\n",
    "    \n",
    "    startTimes = []\n",
    "    usedCurves = []\n",
    "    for i in range(len(pointingInfo)):\n",
    "        \n",
    "        if (pointingInfo[i][0] > date_obs-offset and pointingInfo[i][0] < date_obs+offset and \n",
    "            pointingInfo[i][1] not in usedCurves):\n",
    "            \n",
    "            startTimes.append(pointingInfo[i][0])\n",
    "            \n",
    "            if pointingInfo[i][1] != '0':\n",
    "                usedCurves.append(pointingInfo[i][1])\n",
    "                \n",
    "        elif (i < len(pointingInfo)-1 and pointingInfo[i+1][0] > date_obs-offset and \n",
    "              pointingInfo[i+1][0] < date_obs+offset and pointingInfo[i][1] not in usedCurves):\n",
    "            \n",
    "            startTimes.append(date_obs-offset)\n",
    "            \n",
    "            if pointingInfo[i][1] != '0':\n",
    "                usedCurves.append(pointingInfo[i][1])\n",
    "                       \n",
    "    \n",
    "    metadata= []\n",
    "    \n",
    "    for i in range(len(startTimes)):\n",
    "        if i < len(startTimes)-1:\n",
    "            try:\n",
    "                metadata.append(getXrtPointings(startTimes[i],startTimes[i+1])[0])  #pointingTime+datetime.timedelta(minutes=60))[0])\n",
    "            except IndexError:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                metadata.append(getXrtPointings(startTimes[i],date_obs+offset)[0])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    fig = plt.figure(figsize=(15,15),frameon=False)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    for e in metadata:\n",
    "        if e[2] == e[3]:\n",
    "            xLeft = e[0]-(e[2]*.97*e[4])/2\n",
    "            yBottom = e[1]-(e[3]*.97*e[4])/2\n",
    "            xSize = e[2]*e[4]*.97\n",
    "            ySize = e[3]*e[4]*.97\n",
    "            ax.add_patch(Rectangle((xLeft,yBottom), xSize, ySize, fill=False,color='white',linewidth=3,\n",
    "                                        path_effects=[PathEffects.withStroke(linewidth=5,foreground=\"black\")]))\n",
    "    \n",
    "    flareResponses = [] #flares that triggered XRT flare response\n",
    "    for flare in getFlareModeInfo(date_obs-offset,date_obs+offset):\n",
    "        flareImages = getXrtPointings(flare[0],flare[1],first=False)\n",
    "        if flareImages != []:\n",
    "            flareImages[0].append(len(flareImages))\n",
    "            flareResponses.append(flareImages[0])\n",
    "    \n",
    "    for group in groupFlareResData(flareResponses):\n",
    "        xLeft = group[0]-(group[2]*.97*group[4])/2\n",
    "        yBottom = group[1]-(group[3]*.97*group[4])/2\n",
    "        xSize = group[2]*group[4]*.97\n",
    "        ySize = group[3]*group[4]*.97\n",
    "        if group[6] == 1:\n",
    "            label = str(group[5])\n",
    "        else:\n",
    "            label = str(group[6])+'-'+str(group[5])\n",
    "            \n",
    "        ax.add_patch(Rectangle((xLeft,yBottom), xSize, ySize, fill=False,color='yellow',linewidth=3,\n",
    "                                        path_effects=[PathEffects.withStroke(linewidth=5,foreground=\"black\")]))\n",
    "        \n",
    "        if xLeft > -1075 and yBottom > -1075:\n",
    "            ax.text(xLeft+10,yBottom+10,label,color='yellow',size=28,\n",
    "                     path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "        elif xLeft < -1075 and yBottom > -1075:\n",
    "            ax.text(xLeft+xSize-38*len(label)-10,yBottom+10,label,color='yellow',size=28,\n",
    "                     path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "        else:\n",
    "            ax.text(xLeft+10,yBottom+ySize-38*len(label)-10,label,color='yellow',size=28,\n",
    "                     path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "    \n",
    "    global flareCatalog\n",
    "    flares = []\n",
    "    for flare in flareCatalog:\n",
    "        if date_obs-offset < flare[0] and flare[0] < date_obs+offset:\n",
    "            flares.append(flare)\n",
    "    \n",
    "    for group in groupFlareCatData(flares):\n",
    "        xLeft = group[1] - 20*len(group[3])\n",
    "        yBottom = group[2] - 40*group[0]\n",
    "        label = group[3]\n",
    "        for flareClass in group[4:]:\n",
    "            label = label +'\\n'+flareClass\n",
    "\n",
    "        ax.text(xLeft,yBottom,label,color='yellow',size=28,\n",
    "                 path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "    \n",
    "    map1 = sunpy.map.Map(np.log10(np.maximum(file1[0].data,1)),dict(file1[0].header))\n",
    "\n",
    "    map1.plot(vmin=0.5,vmax=4)\n",
    "\n",
    "\n",
    "    ax.text(-1040,890,'XRT',color='white',size=56,\n",
    "                                        path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "    ax.text(-1040,820,file1[0].header['ec_fw1_'] + ' / ' + file1[0].header['ec_fw2_'],color='white',size=28,\n",
    "                                        path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "    ax.text(-1040,-1015,file1[0].header['date_obs'],color='white',size=28,\n",
    "                                        path_effects=[PathEffects.withStroke(linewidth=2,foreground=\"black\")])\n",
    "    \n",
    "    fig.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "    \n",
    "    #fig.savefig('/home/gwoelki/'+datetime.datetime.strftime(date_obs,'%Y-%m-%dT%H%M%S')+'.jpg',\n",
    "    #            dpi=96)\n",
    "    plt.show()\n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    file1.close()\n",
    "    gc.collect()\n",
    "\n",
    "    return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_f = datetime.datetime.utcnow() #datetime.datetime.strptime('2015-06-10 06:00','%Y-%m-%d %H:%M')\n",
    "t_i = t_f - datetime.timedelta(days=365)\n",
    "\n",
    "flareCatalog = getHinodeFlareCat(t_i,t_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This iterates through all synoptics of the specified filter in the t_i to t_f time range\n",
    "\n",
    "for synoptic in getSynoptics(t_i,t_f):\n",
    "    displaySynoptic(synoptic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
